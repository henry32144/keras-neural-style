{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda3\\envs\\neural-style\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import pdb\n",
    "import functools\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from scipy.misc import imread, imresize, imsave, fromimage, toimage\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.python.layers import utils\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, ZeroPadding2D,merge, Lambda, concatenate\n",
    "from keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D,Deconvolution2D \n",
    "from keras.layers.convolutional import Conv2D,UpSampling2D,Cropping2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.layers.core import Activation\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import Regularizer\n",
    "\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv_weight = 1e-6\n",
    "\n",
    "IMG_WIDTH=512\n",
    "IMG_HEIGHT=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, IMG_WIDTH=IMG_WIDTH, IMG_HEIGHT=IMG_HEIGHT):\n",
    "    mode = \"RGB\"\n",
    "    img = imread(image_path, mode=mode)\n",
    "\n",
    "    img = imresize(img,(IMG_WIDTH, IMG_HEIGHT)).astype('float32')\n",
    "\n",
    "    # 這個keras內建函式可以幫我們做到上面的轉換\n",
    "    #img = preprocess_input(img)\n",
    "    \n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        img = img.transpose((2, 0, 1)).astype('float32')\n",
    "\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    print(img.shape)\n",
    "    return img\n",
    "\n",
    "def preprocess_image(image_path, IMG_WIDTH=IMG_WIDTH, IMG_HEIGHT=IMG_HEIGHT):\n",
    "    mode = \"RGB\"\n",
    "    img = imread(image_path, mode=mode)\n",
    "\n",
    "    img = imresize(img,(IMG_WIDTH, IMG_HEIGHT)).astype('float32')\n",
    "    \n",
    "    # 這個keras內建函式可以幫我們做到上面的轉換\n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        img = img.transpose((2, 0, 1)).astype('float32')\n",
    "\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    print(img.shape)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x, IMG_WIDTH=IMG_WIDTH, IMG_HEIGHT=IMG_HEIGHT):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        x = x.reshape((3, IMG_WIDTH, IMG_HEIGHT))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    \n",
    "    # BGR -> RGB\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    # 將陣列的值的範圍縮回 0~255，因為處理的結果有可能出現超過這個範圍的數字\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def get_ratio(image):\n",
    "    img = Image.fromarray(image).convert('RGB')\n",
    "    img_WIDTH, img_HEIGHT = img.size\n",
    "    aspect_ratio = float(img_HEIGHT) / img_WIDTH\n",
    "    return aspect_ratio\n",
    "\n",
    "\n",
    "def show(x, IMG_WIDTH=IMG_WIDTH, IMG_HEIGHT=IMG_HEIGHT, save=False, name='', iterate=0):\n",
    "    # 將張量轉回圖片的後處理\n",
    "    img = deprocess_image(x.copy())\n",
    "    \n",
    "    # 取得原圖比例\n",
    "    aspect_ratio = get_ratio(img)  \n",
    "    img_ht = int(IMG_WIDTH * aspect_ratio)\n",
    "    #print(\"Rescaling Image to (%d, %d)\" % (IMG_WIDTH, img_ht))\n",
    "    img = imresize(img, (IMG_WIDTH, img_ht), interp='bilinear')\n",
    "    im = toimage(img)\n",
    "    if save:\n",
    "        filename = 'output/%s_%d.jpg' % (name, iterate)\n",
    "        imsave(filename, im)\n",
    "    else:\n",
    "        plt.imshow(im)\n",
    "    \n",
    "def show_without_deprocess(x, IMG_WIDTH=IMG_WIDTH, IMG_HEIGHT=IMG_HEIGHT, save=False, name='', iterate=0):\n",
    "\n",
    "    x = x.reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    img = np.clip(x, 0, 255).astype('uint8')\n",
    "    \n",
    "    # 取得原圖比例\n",
    "    aspect_ratio = get_ratio(img)  \n",
    "    img_ht = int(IMG_WIDTH * aspect_ratio)\n",
    "    print(\"Rescaling Image to (%d, %d)\" % (IMG_WIDTH, img_ht))\n",
    "    img = imresize(img, (IMG_WIDTH, img_ht), interp='bilinear')\n",
    "    print(img)\n",
    "    im = toimage(img)\n",
    "    if save:\n",
    "        filename = 'output/%s_%d.jpg' % (name, iterate)\n",
    "        imsave(filename, im)\n",
    "    else:\n",
    "        plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InstanceNormalize, self).__init__(**kwargs)\n",
    "        self.epsilon = 1e-5\n",
    "        self.alpha = 0.8\n",
    "            \n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        style_mean, style_variance = tf.nn.moments(style_features, [1,2], keep_dims=True)\n",
    "        content_mean, content_variance = tf.nn.moments(content_features, [1,2], keep_dims=True)\n",
    "        \n",
    "        normalized_content_features = tf.nn.batch_normalization(content_features, content_mean,\n",
    "                                                            content_variance, style_mean, \n",
    "                                                            tf.sqrt(style_variance), self.epsilon)\n",
    "        \n",
    "        normalized_content_features = self.alpha * normalized_content_features + (1 - self.alpha) * content_features\n",
    "        return normalized_content_features\n",
    "                                                 \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class InstanceNormalize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InstanceNormalize, self).__init__(**kwargs)\n",
    "        self.epsilon = 1e-3\n",
    "            \n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
    "        return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, self.epsilon)))\n",
    "\n",
    "                                                 \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), dim_ordering='default', **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        self.padding = padding\n",
    "        if isinstance(padding, dict):\n",
    "            if set(padding.keys()) <= {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}:\n",
    "                self.top_pad = padding.get('top_pad', 0)\n",
    "                self.bottom_pad = padding.get('bottom_pad', 0)\n",
    "                self.left_pad = padding.get('left_pad', 0)\n",
    "                self.right_pad = padding.get('right_pad', 0)\n",
    "            else:\n",
    "                raise ValueError('Unexpected key found in `padding` dictionary. '\n",
    "                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", '\n",
    "                                 '\"left_pad\", \"right_pad\"}.'\n",
    "                                 'Found: ' + str(padding.keys()))\n",
    "        else:\n",
    "            padding = tuple(padding)\n",
    "            if len(padding) == 2:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[0]\n",
    "                self.left_pad = padding[1]\n",
    "                self.right_pad = padding[1]\n",
    "            elif len(padding) == 4:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[1]\n",
    "                self.left_pad = padding[2]\n",
    "                self.right_pad = padding[3]\n",
    "            else:\n",
    "                raise TypeError('`padding` should be tuple of int '\n",
    "                                'of length 2 or 4, or dict. '\n",
    "                                'Found: ' + str(padding))\n",
    "\n",
    "        if dim_ordering not in {'tf'}:\n",
    "            raise ValueError('dim_ordering must be in {tf}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=4)] \n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        top_pad=self.top_pad\n",
    "        bottom_pad=self.bottom_pad\n",
    "        left_pad=self.left_pad\n",
    "        right_pad=self.right_pad        \n",
    "        \n",
    "        paddings = [[0,0],[left_pad,right_pad],[top_pad,bottom_pad],[0,0]]\n",
    "\n",
    "        \n",
    "        return tf.pad(x,paddings, mode='REFLECT', name=None)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        if self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1] + self.top_pad + self.bottom_pad if input_shape[1] is not None else None\n",
    "            cols = input_shape[2] + self.left_pad + self.right_pad if input_shape[2] is not None else None\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding}\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))     \n",
    "    \n",
    "    \n",
    "class UnPooling2D(UpSampling2D):\n",
    "    def __init__(self, size=(2, 2)):\n",
    "        super(UnPooling2D, self).__init__(size)\n",
    "\n",
    "  \n",
    "    def call(self, x, mask=None):\n",
    "        shapes = x.get_shape().as_list() \n",
    "        w = self.size[0] * shapes[1]\n",
    "        h = self.size[1] * shapes[2]\n",
    "        return tf.image.resize_nearest_neighbor(x, (w,h))\n",
    "    \n",
    "class VGGNormalize(Layer):\n",
    "    '''\n",
    "    Custom layer to subtract the outputs of previous layer by 120,\n",
    "    to normalize the inputs to the VGG network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VGGNormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # No exact substitute for set_subtensor in tensorflow\n",
    "        # So we subtract an approximate value       \n",
    "        \n",
    "        # 'RGB'->'BGR'\n",
    "        x = preprocess_input(x)\n",
    "        return x\n",
    "   \n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "class TanhNormalize(Layer):\n",
    "    '''\n",
    "    Custom layer to subtract the outputs of previous layer by 120,\n",
    "    to normalize the inputs to the VGG network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhNormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # No exact substitute for set_subtensor in tensorflow\n",
    "        # So we subtract an approximate value       \n",
    "        \n",
    "        # 'RGB'->'BGR'\n",
    "        x = (x + 1) * (255.0 / 2)\n",
    "        return x\n",
    "   \n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InstanceNormalization(Layer):\n",
    "    def __init__(self,\n",
    "                 axis=None,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ndim = len(input_shape)\n",
    "        if self.axis == 0:\n",
    "            raise ValueError('Axis cannot be zero')\n",
    "\n",
    "        if (self.axis is not None) and (ndim == 2):\n",
    "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=ndim)\n",
    "\n",
    "        if self.axis is None:\n",
    "            shape = (1,)\n",
    "        else:\n",
    "            shape = (input_shape[self.axis],)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        reduction_axes = list(range(0, len(input_shape)))\n",
    "\n",
    "        if (self.axis is not None):\n",
    "            del reduction_axes[self.axis]\n",
    "\n",
    "        del reduction_axes[0]\n",
    "\n",
    "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
    "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
    "        normed = (inputs - mean) / stddev\n",
    "\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        if self.axis is not None:\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            normed = normed * broadcast_gamma\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            normed = normed + broadcast_beta\n",
    "        return normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(InstanceNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_encode_net(input_shape):\n",
    "    \n",
    "    #vgg = VGG19(include_top=False, input_tensor=input_tensor)\n",
    "    vgg = VGG19(include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    encode_layer = Model(vgg.input, vgg.layers[7].output)\n",
    "    \n",
    "    for layer in encode_layer.layers[:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    encode_layer.compile(optimizer='adam', loss='mse')\n",
    "    return encode_layer\n",
    "\n",
    "def build_encode_net_with_swap_3_1(input_shape):\n",
    "    \n",
    "    content_input = Input(shape=input_shape, name='content_input')\n",
    "    style_input = Input(shape=input_shape, name='style_input')\n",
    "    x = concatenate([content_input, style_input], axis=0)\n",
    "    \n",
    "    vgg = VGG19(include_top=False, input_tensor=x)\n",
    "    \n",
    "    swapped = Lambda(style_swap_layer, output_shape=(64, 64, 256))(vgg.layers[-15].output)\n",
    "    \n",
    "    encode_layer = Model([content_input, style_input], swapped)\n",
    "    \n",
    "    for layer in encode_layer.layers[:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    encode_layer.compile(optimizer='adam', loss='mse')\n",
    "    return encode_layer\n",
    "\n",
    "def build_encode_net_with_swap(input_shape):\n",
    "    \n",
    "    content_input = Input(shape=input_shape, name='content_input')\n",
    "    style_input = Input(shape=input_shape, name='style_input')\n",
    "    x = concatenate([content_input, style_input], axis=0)\n",
    "    \n",
    "    vgg = VGG19(include_top=False, input_tensor=x)\n",
    "    \n",
    "    swapped = Lambda(benben_swap_layer)(vgg.layers[-13].output)\n",
    "    \n",
    "    encode_layer = Model([content_input, style_input], swapped)\n",
    "    \n",
    "    for layer in encode_layer.layers[:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    encode_layer.compile(optimizer='adam', loss='mse')\n",
    "    return encode_layer\n",
    "\n",
    "def build_encode_net_vgg16_3_1(input_shape):\n",
    "    \n",
    "    content_input = Input(shape=input_shape, name='content_input')\n",
    "    style_input = Input(shape=input_shape, name='style_input')\n",
    "    x = concatenate([content_input, style_input], axis=0)\n",
    "    \n",
    "    vgg = VGG16(include_top=False, input_tensor=x)\n",
    "    \n",
    "    swapped = Lambda(style_swap_layer, output_shape=(64, 64, 256))(vgg.layers[-12].output)\n",
    "    \n",
    "    encode_layer = Model([content_input, style_input], swapped)\n",
    "    \n",
    "    for layer in encode_layer.layers[:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    encode_layer.compile(optimizer='adam', loss='mse')\n",
    "    return encode_layer\n",
    "\n",
    "def build_encode_net_with_swap_3_3(input_shape):\n",
    "    \n",
    "    content_input = Input(shape=input_shape, name='content_input')\n",
    "    style_input = Input(shape=input_shape, name='style_input')\n",
    "    x = concatenate([content_input, style_input], axis=0)\n",
    "    \n",
    "    vgg = VGG19(include_top=False, input_tensor=x)\n",
    "    \n",
    "    swapped = Lambda(wct_style_swap, output_shape=(64, 64, 256))(vgg.layers[-13].output)\n",
    "    \n",
    "    encode_layer = Model([content_input, style_input], swapped)\n",
    "    \n",
    "    for layer in encode_layer.layers[:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    encode_layer.compile(optimizer='adam', loss='mse')\n",
    "    return encode_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_in_relu(nb_filter, nb_row, nb_col,stride):   \n",
    "    def conv_func(x):\n",
    "        x = Conv2D(nb_filter, (nb_row, nb_col), strides=stride , padding='same')(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "    return conv_func\n",
    "\n",
    "def res_conv(nb_filter, nb_row, nb_col,stride=(1,1)):\n",
    "    def _res_func(x):\n",
    "        #identity = Cropping2D(cropping=((2,2),(2,2)))(x)\n",
    "\n",
    "        a = Conv2D(nb_filter, (nb_row, nb_col), strides=stride, padding='same')(x)\n",
    "        a = InstanceNormalize()(a)\n",
    "        #a = LeakyReLU(0.2)(a)\n",
    "        a = Activation(\"relu\")(a)\n",
    "        a = Conv2D(nb_filter, (nb_row, nb_col), strides=stride, padding='same')(a)\n",
    "        y = InstanceNormalize()(a)\n",
    "\n",
    "        return  add([x, y])\n",
    "\n",
    "    return _res_func\n",
    "\n",
    "def dconv_bn_nolinear(nb_filter, nb_row, nb_col,stride=(2,2),activation=\"relu\"):\n",
    "    def _dconv_bn(x):\n",
    "        x = UnPooling2D(size=stride)(x)\n",
    "        x = ReflectionPadding2D(padding=stride)(x)\n",
    "        x = Conv2D(nb_filter, (nb_row, nb_col), padding='valid')(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = Activation(activation)(x)\n",
    "        return x\n",
    "    return _dconv_bn\n",
    "\n",
    "\n",
    "def add_total_variation_loss(transform_output_layer,weight):\n",
    "    # Total Variation Regularization\n",
    "    layer = transform_output_layer  # Output layer\n",
    "    tv_regularizer = TVRegularizer(weight)(layer)\n",
    "    layer.add_loss(tv_regularizer)\n",
    "    \n",
    "    \n",
    "class TVRegularizer(Regularizer):\n",
    "    \"\"\" Enforces smoothness in image output. \"\"\"\n",
    "\n",
    "    def __init__(self, weight=1.0):\n",
    "        self.weight = weight\n",
    "        self.uses_learning_phase = False\n",
    "        super(TVRegularizer, self).__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        assert K.ndim(x.output) == 4\n",
    "        x_out = x.output\n",
    "        \n",
    "        shape = K.shape(x_out)\n",
    "        img_width, img_height,channel = (shape[1],shape[2], shape[3])\n",
    "        size = img_width * img_height * channel \n",
    "        if K.image_dim_ordering() == 'th':\n",
    "            a = K.square(x_out[:, :, :img_width - 1, :img_height - 1] - x_out[:, :, 1:, :img_height - 1])\n",
    "            b = K.square(x_out[:, :, :img_width - 1, :img_height - 1] - x_out[:, :, :img_width - 1, 1:])\n",
    "        else:\n",
    "            a = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, 1:, :img_height - 1, :])\n",
    "            b = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, :img_width - 1, 1:, :])\n",
    "        loss = self.weight * K.sum(K.pow(a + b, 1.25)) \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wct_style_swap(x, alpha=0.8, patch_size=3, stride=1, eps=1e-8):\n",
    "    '''Modified Whiten-Color Transform that performs style swap on whitened content/style encodings before coloring\n",
    "       Assume that content/style encodings have shape 1xHxWxC\n",
    "    '''\n",
    "    \n",
    "    content = K.expand_dims(x[0], 0)\n",
    "    style = K.expand_dims(x[1], 0)\n",
    "    \n",
    "    content_t = tf.transpose(tf.squeeze(content), (2, 0, 1))\n",
    "    style_t = tf.transpose(tf.squeeze(style), (2, 0, 1))\n",
    "\n",
    "    Cc, Hc, Wc = tf.unstack(tf.shape(content_t))\n",
    "    Cs, Hs, Ws = tf.unstack(tf.shape(style_t))\n",
    "\n",
    "    # CxHxW -> CxH*W\n",
    "    content_flat = tf.reshape(content_t, (Cc, Hc*Wc))\n",
    "    style_flat = tf.reshape(style_t, (Cs, Hs*Ws))\n",
    "\n",
    "    # Content covariance\n",
    "    mc = tf.reduce_mean(content_flat, axis=1, keep_dims=True)\n",
    "    fc = content_flat - mc\n",
    "    fcfc = tf.matmul(fc, fc, transpose_b=True) / (tf.cast(Hc*Wc, tf.float32) - 1.) + tf.eye(Cc)*eps\n",
    "\n",
    "    # Style covariance\n",
    "    ms = tf.reduce_mean(style_flat, axis=1, keep_dims=True)\n",
    "    fs = style_flat - ms\n",
    "    fsfs = tf.matmul(fs, fs, transpose_b=True) / (tf.cast(Hs*Ws, tf.float32) - 1.) + tf.eye(Cs)*eps\n",
    "\n",
    "    # tf.svd is slower on GPU, see https://github.com/tensorflow/tensorflow/issues/13603\n",
    "    with tf.device('/cpu:0'):  \n",
    "        Sc, Uc, _ = tf.svd(fcfc)\n",
    "        Ss, Us, _ = tf.svd(fsfs)\n",
    "\n",
    "    ## Uncomment to perform SVD for content/style with np in one call\n",
    "    ## This is slower than CPU tf.svd but won't segfault for ill-conditioned matrices\n",
    "    # @jit\n",
    "    # def np_svd(content, style):\n",
    "    #     '''tf.py_func helper to run SVD with NumPy for content/style cov tensors'''\n",
    "    #     Uc, Sc, _ = np.linalg.svd(content)\n",
    "    #     Us, Ss, _ = np.linalg.svd(style)\n",
    "    #     return Uc, Sc, Us, Ss\n",
    "    # Uc, Sc, Us, Ss = tf.py_func(np_svd, [fcfc, fsfs], [tf.float32, tf.float32, tf.float32, tf.float32])\n",
    "    \n",
    "    k_c = tf.reduce_sum(tf.cast(tf.greater(Sc, 1e-5), tf.int32))\n",
    "    k_s = tf.reduce_sum(tf.cast(tf.greater(Ss, 1e-5), tf.int32))\n",
    "\n",
    "    ### Whiten content\n",
    "    Dc = tf.diag(tf.pow(Sc[:k_c], -0.5))\n",
    "\n",
    "    fc_hat = tf.matmul(tf.matmul(tf.matmul(Uc[:,:k_c], Dc), Uc[:,:k_c], transpose_b=True), fc)\n",
    "\n",
    "    # Reshape before passing to style swap, CxH*W -> 1xHxWxC\n",
    "    whiten_content = tf.expand_dims(tf.transpose(tf.reshape(fc_hat, [Cc,Hc,Wc]), [1,2,0]), 0)\n",
    "\n",
    "    ### Whiten style before swapping\n",
    "    Ds = tf.diag(tf.pow(Ss[:k_s], -0.5))\n",
    "    whiten_style = tf.matmul(tf.matmul(tf.matmul(Us[:,:k_s], Ds), Us[:,:k_s], transpose_b=True), fs)\n",
    "    # Reshape before passing to style swap, CxH*W -> 1xHxWxC\n",
    "    whiten_style = tf.expand_dims(tf.transpose(tf.reshape(whiten_style, [Cs,Hs,Ws]), [1,2,0]), 0)\n",
    "\n",
    "    ### Style swap whitened encodings\n",
    "    #ss_feature = ori_style_swap_layer(whiten_content, whiten_style, patch_size, stride)\n",
    "    \n",
    "    ###############################################\n",
    "    nC = tf.shape(whiten_style)[-1]  # Num channels of input content feature and style-swapped output\n",
    "\n",
    "    ### Extract patches from style image that will be used for conv/deconv layers\n",
    "    style_patches = tf.extract_image_patches(whiten_style, [1,patch_size,patch_size,1], [1,stride,stride,1], [1,1,1,1], 'VALID')\n",
    "    before_reshape = tf.shape(style_patches)  # NxRowsxColsxPatch_size*Patch_size*nC\n",
    "    style_patches = tf.reshape(style_patches, [before_reshape[1]*before_reshape[2],patch_size,patch_size,nC])\n",
    "    style_patches = tf.transpose(style_patches, [1,2,3,0])  # Patch_sizexPatch_sizexIn_CxOut_c\n",
    "\n",
    "    # Normalize each style patch\n",
    "    style_patches_norm = tf.nn.l2_normalize(style_patches, dim=3)\n",
    "\n",
    "    # Compute cross-correlation/nearest neighbors of patches by using style patches as conv filters\n",
    "    ss_enc = tf.nn.conv2d(whiten_content,\n",
    "                          style_patches_norm,\n",
    "                          [1,stride,stride,1],\n",
    "                          'VALID')\n",
    "\n",
    "    # For each spatial position find index of max along channel/patch dim  \n",
    "    ss_argmax = tf.argmax(ss_enc, axis=3)\n",
    "    encC = tf.shape(ss_enc)[-1]  # Num channels in intermediate conv output, same as # of patches\n",
    "    \n",
    "    # One-hot encode argmax with same size as ss_enc, with 1's in max channel idx for each spatial pos\n",
    "    ss_oh = tf.one_hot(ss_argmax, encC, 1., 0., 3)\n",
    "\n",
    "    # Calc size of transposed conv out\n",
    "    deconv_out_H = utils.deconv_output_length(tf.shape(ss_oh)[1], patch_size, 'valid', stride)\n",
    "    deconv_out_W = utils.deconv_output_length(tf.shape(ss_oh)[2], patch_size, 'valid', stride)\n",
    "    deconv_out_shape = tf.stack([1,deconv_out_H,deconv_out_W,nC])\n",
    "\n",
    "    # Deconv back to original content size with highest matching (unnormalized) style patch swapped in for each content patch\n",
    "    ss_dec = tf.nn.conv2d_transpose(ss_oh,\n",
    "                                    style_patches,\n",
    "                                    deconv_out_shape,\n",
    "                                    [1,stride,stride,1],\n",
    "                                    'VALID')\n",
    "\n",
    "    ### Interpolate to average overlapping patch locations\n",
    "    ss_oh_sum = tf.reduce_sum(ss_oh, axis=3, keep_dims=True)\n",
    "\n",
    "    filter_ones = tf.ones([patch_size,patch_size,1,1], dtype=tf.float32)\n",
    "    \n",
    "    deconv_out_shape = tf.stack([1,deconv_out_H,deconv_out_W,1])  # Same spatial size as ss_dec with 1 channel\n",
    "\n",
    "    counting = tf.nn.conv2d_transpose(ss_oh_sum,\n",
    "                                         filter_ones,\n",
    "                                         deconv_out_shape,\n",
    "                                         [1,stride,stride,1],\n",
    "                                         'VALID')\n",
    "\n",
    "    counting = tf.tile(counting, [1,1,1,nC])  # Repeat along channel dim to make same size as ss_dec\n",
    "\n",
    "    ss_feature = tf.divide(ss_dec, counting)\n",
    "    ###############################################\n",
    "    \n",
    "    # HxWxC -> CxH*W\n",
    "    ss_feature = tf.transpose(tf.reshape(ss_feature, [Hc*Wc,Cc]), [1,0])\n",
    "\n",
    "    ### Color style-swapped encoding with style \n",
    "    Ds_sq = tf.diag(tf.pow(Ss[:k_s], 0.5))\n",
    "    fcs_hat = tf.matmul(tf.matmul(tf.matmul(Us[:,:k_s], Ds_sq), Us[:,:k_s], transpose_b=True), ss_feature)\n",
    "    fcs_hat = fcs_hat + ms\n",
    "\n",
    "    ### Blend style-swapped & colored encoding with original content encoding\n",
    "    blended = alpha * fcs_hat + (1 - alpha) * (fc + mc)\n",
    "    # CxH*W -> CxHxW\n",
    "    blended = tf.reshape(blended, (Cc,Hc,Wc))\n",
    "    # CxHxW -> 1xHxWxC\n",
    "    blended = tf.expand_dims(tf.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "    return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def style_swap_layer(x, patch_size=3, stride=1):\n",
    "    '''Efficiently swap content feature patches with nearest-neighbor style patches\n",
    "       Original paper: https://arxiv.org/abs/1612.04337\n",
    "       Adapted from: https://github.com/rtqichen/style-swap/blob/master/lib/NonparametricPatchAutoencoderFactory.lua\n",
    "    '''\n",
    "    content = K.expand_dims(x[0], 0)\n",
    "    style = K.expand_dims(x[1], 0)\n",
    "        \n",
    "        \n",
    "    nC = style.shape[-1]  # Num channels of input content feature and style-swapped output\n",
    "\n",
    "    content_t = tf.transpose(tf.squeeze(content), (2, 0, 1))\n",
    "    style_t = tf.transpose(tf.squeeze(style), (2, 0, 1))\n",
    "\n",
    "    Cc, Hc, Wc = tf.unstack(content_t.shape)\n",
    "    Cs, Hs, Ws = tf.unstack(style_t.shape)\n",
    "\n",
    "\n",
    "    ### Extract patches from style image that will be used for conv/deconv layers\n",
    "    style_patches = tf.extract_image_patches(style, [1, patch_size, patch_size, 1], \n",
    "                                                 [1, stride, stride, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "    before_reshape = style_patches.shape  # NxRowsxColsxPatch_size*Patch_size*nC\n",
    "\n",
    "    style_patches = tf.reshape(style_patches, [before_reshape[1]*before_reshape[2], patch_size, patch_size, nC])\n",
    "\n",
    "    style_patches = tf.transpose(style_patches, [1, 2, 3, 0])  # Patch_sizexPatch_sizexIn_CxOut_c\n",
    "\n",
    "    # Normalize each style patch\n",
    "    style_patches_norm = tf.nn.l2_normalize(style_patches, dim=3)\n",
    "\n",
    "    # Compute cross-correlation/nearest neighbors of patches by using style patches as conv filters\n",
    "    ss_enc = tf.nn.conv2d(content,\n",
    "                              style_patches_norm,\n",
    "                              [1, stride, stride, 1],\n",
    "                              'VALID')\n",
    "\n",
    "    # For each spatial position find index of max along channel/patch dim  \n",
    "    ss_argmax = tf.argmax(ss_enc, axis=3)\n",
    "    encC = ss_enc.shape[-1]  # Num channels in intermediate conv output, same as # of patches\n",
    "\n",
    "    # One-hot encode argmax with same size as ss_enc, with 1's in max channel idx for each spatial pos\n",
    "    ss_oh = tf.one_hot(ss_argmax, encC, 1., 0., 3)\n",
    "\n",
    "    # Calc size of transposed conv out\n",
    "    deconv_out_H = utils.deconv_output_length(ss_oh.shape[1], patch_size, 'valid', stride)\n",
    "    deconv_out_W = utils.deconv_output_length(ss_oh.shape[2], patch_size, 'valid', stride)\n",
    "    deconv_out_shape = tf.stack([1, deconv_out_H, deconv_out_W, nC])\n",
    "\n",
    "\n",
    "    # Deconv back to original content size with highest matching (unnormalized) style patch swapped in for each content patch\n",
    "    ss_dec = tf.nn.conv2d_transpose(ss_oh,\n",
    "                                        style_patches,\n",
    "                                        deconv_out_shape,\n",
    "                                        [1, stride, stride, 1],\n",
    "                                        'VALID')\n",
    "\n",
    "    ### Interpolate to average overlapping patch locations\n",
    "    ss_oh_sum = tf.reduce_sum(ss_oh, axis=3, keep_dims=True)\n",
    "\n",
    "    filter_ones = tf.ones([patch_size, patch_size, 1, 1], dtype=tf.float32)\n",
    "\n",
    "    deconv_out_shape = tf.stack([1, deconv_out_H, deconv_out_W, 1])  # Same spatial size as ss_dec with 1 channel\n",
    "\n",
    "    counting = tf.nn.conv2d_transpose(ss_oh_sum,\n",
    "                                             filter_ones,\n",
    "                                             deconv_out_shape,\n",
    "                                             [1, stride, stride,1],\n",
    "                                             'VALID')\n",
    "\n",
    "    counting = tf.tile(counting, [1, 1, 1, nC])  # Repeat along channel dim to make same size as ss_dec\n",
    "\n",
    "    interpolated_dec = tf.divide(ss_dec, counting)\n",
    "        \n",
    "    return interpolated_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benben_swap_layer(x, cell_size = 3):\n",
    "    content_feature = tf.expand_dims(x[0], 0)\n",
    "    style_feature = tf.expand_dims(x[1], 0)\n",
    "    \n",
    "    style_amount = style_feature.get_shape()[0].value\n",
    "    \n",
    "    print(\"style_amount:\", style_amount)\n",
    "    \n",
    "    rows = tf.split(style_feature, num_or_size_splits=list(\n",
    "            [cell_size] * (style_feature.get_shape()[1].value // cell_size) + [style_feature.get_shape()[1].value % cell_size]), axis=1)[:-1]\n",
    "    cells = [tf.split(row, num_or_size_splits=list(\n",
    "            [cell_size] * (style_feature.get_shape()[2].value // cell_size) + [style_feature.get_shape()[2].value % cell_size]), axis=2)[:-1]\n",
    "                 for row in rows]\n",
    "\n",
    "    print(\"row shape:\" , np.array(cells).shape)\n",
    "    \n",
    "    stacked_cells = [tf.stack(row_cell, axis=4) for row_cell in cells]\n",
    "    \n",
    "    print(\"stacked_cells:\" , np.array(stacked_cells).shape)\n",
    "    \n",
    "    filters = tf.concat(stacked_cells, axis=-1)\n",
    "    \n",
    "    print(\"filters:\" , filters.get_shape())\n",
    "    \n",
    "    swaped_list = []\n",
    "    for style_filter in tf.unstack(filters, axis=0, num=style_amount):\n",
    "        \n",
    "        height = tf.shape(content_feature)[1]\n",
    "        width = tf.shape(content_feature)[2]\n",
    "        #print(style_filter)\n",
    "        normalized_filters = tf.nn.l2_normalize(style_filter, dim=3)\n",
    "        \n",
    "        print(\"normalized_filters:\" , normalized_filters.get_shape())\n",
    "        print(\"content_feature:\" , content_feature.get_shape())\n",
    "        \n",
    "        \"\"\" change the strides to see difference\"\"\"\n",
    "        similarity = tf.nn.conv2d(content_feature, normalized_filters, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "        arg_max_filter = tf.argmax(similarity, axis=-1)\n",
    "        one_hot_filter = tf.one_hot(arg_max_filter, depth=similarity.get_shape()[-1].value)\n",
    "\n",
    "        swap = tf.nn.conv2d_transpose(one_hot_filter, style_filter, output_shape=tf.shape(content_feature),\n",
    "                                      strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        \n",
    "        swaped_list.append(swap / 9.0)\n",
    "    \n",
    "    layer_out = tf.concat(swaped_list, axis=0)\n",
    "    print(layer_out)\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InverseNet_4(feature):\n",
    "    ## feature = shape of content concatenate with style\n",
    "    \n",
    "    content_input = Input(shape=feature, name='content_input')\n",
    "    style_input = Input(shape=feature, name='style_input')\n",
    "    x = concatenate([content_input, style_input], axis=0)\n",
    "    \n",
    "    x = Lambda(wct_style_swap, output_shape=feature)(x)\n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    inverse_net_output = Conv2D(3, (3, 3), padding='same', name='inverse_net_output')(x)\n",
    "    \n",
    "    model = Model(inputs=[content_input, style_input], outputs=inverse_net_output)\n",
    "   \n",
    "    add_total_variation_loss(model.layers[-1], tv_weight)\n",
    "      \n",
    "    return model\n",
    "\n",
    "def InverseNet_5(feature):\n",
    "    ## feature = shape of content concatenate with style\n",
    "    \n",
    "    swapped_input = Input(shape=feature, name='swapped_input')\n",
    "    \n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(swapped_input)\n",
    "    x = UnPooling2D()(x)\n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    x = UnPooling2D()(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    inverse_net_output = Conv2D(3, (3, 3), padding='same', name='inverse_net_output')(x)\n",
    "    \n",
    "    model = Model(inputs=[swapped_input], outputs=inverse_net_output)\n",
    "   \n",
    "    add_total_variation_loss(model.layers[-1], tv_weight)\n",
    "      \n",
    "    return model\n",
    "\n",
    "def InverseNet_3_3(feature):\n",
    "    ## feature = shape of content concatenate with style\n",
    "    \n",
    "    swapped_input = Input(shape=feature, name='swapped_input')\n",
    "    \n",
    "    x = conv_in_relu(256, 3, 3, stride=(1,1))(swapped_input)\n",
    "    x = conv_in_relu(256, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(256, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(256, 3, 3, stride=(1,1))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    \n",
    "    inverse_net_output = Conv2D(3, (3, 3), padding='same', name='inverse_net_output')(x)\n",
    "    \n",
    "    model = Model(inputs=[swapped_input], outputs=inverse_net_output)\n",
    "   \n",
    "    add_total_variation_loss(model.layers[-1], tv_weight)\n",
    "      \n",
    "    return model\n",
    "\n",
    "def InverseNet_3_1(feature):\n",
    "    ## feature = shape of content concatenate with style\n",
    "    \n",
    "    swapped_input = Input(shape=feature, name='swapped_input')\n",
    "    \n",
    "    x = conv_in_relu(256, 3, 3, stride=(1,1))(swapped_input)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    \n",
    "    inverse_net_output = Conv2D(3, (3, 3), padding='same', name='inverse_net_output')(x)\n",
    "              \n",
    "    model = Model(inputs=[swapped_input], outputs=inverse_net_output)\n",
    "   \n",
    "    add_total_variation_loss(model.layers[-1], tv_weight)\n",
    "      \n",
    "    return model\n",
    "\n",
    "def InverseNet_3_3_res(feature):\n",
    "    ## feature = shape of content concatenate with style\n",
    "    \n",
    "    swapped_input = Input(shape=feature, name='swapped_input')\n",
    "    \n",
    "    x = conv_in_relu(256, 3, 3, stride=(1,1))(swapped_input)\n",
    "    x = res_conv(256, 3, 3, stride=(1,1))(x)\n",
    "    x = res_conv(256, 5, 5, stride=(1,1))(x)\n",
    "    x = res_conv(256, 7, 7, stride=(1,1))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(128, 5, 5, stride=(1,1))(x)\n",
    "    #x = conv_in_relu(128, 3, 3, stride=(1,1))(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    x = conv_in_relu(64, 5, 5, stride=(1,1))(x)\n",
    "    x = conv_in_relu(64, 3, 3, stride=(1,1))(x)\n",
    "    \n",
    "    inverse_net_output = Conv2D(3, (3, 3), padding='same', name='inverse_net_output',activation=\"tanh\")(x)\n",
    "    \n",
    "    x = TanhNormalize()(inverse_net_output)\n",
    "\n",
    "              \n",
    "    model = Model(inputs=[swapped_input], outputs=x)\n",
    "   \n",
    "    add_total_variation_loss(model.layers[-1], tv_weight)\n",
    "      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "test_content_image_1 = 'images/content/101.jpg'\n",
    "handsome = 'images/content/handsomeman.jpg'\n",
    "lucechapel = 'images/content/lucechapel.jpg'\n",
    "cat = 'images/content/gilbert.jpg'\n",
    "bike = 'images/content/bike.jpg'\n",
    "uchiha = \"images/content/uchiha.jpg\"\n",
    "autumn = \"images/content/autumn.jpg\"\n",
    "street = \"images/content/street.jpg\"\n",
    "\n",
    "snow = 'images/style/snow.jpg'\n",
    "starry_night = 'images/style/starry_night.jpg'\n",
    "crystal = 'images/style/crystal.jpg'\n",
    "la_muse = 'images/style/la_muse.jpg'\n",
    "udnie = 'images/style/udnie.jpg'\n",
    "water = 'images/style/water.jpg'\n",
    "night = 'images/style/101night.jpg'\n",
    "chingmin = 'images/style/Chingmin.jpg'\n",
    "colorhole = 'images/style/colorhole.jpg'\n",
    "des_glaneuses = 'images/style/des_glaneuses.jpg'\n",
    "jingdo = 'images/style/jingdo.jpg'\n",
    "monalisa = 'images/style/monalisa.jpg'\n",
    "mountainwater = 'images/style/mountainwater.jpg'\n",
    "picassoself = 'images/style/picassoself.jpg'\n",
    "wave_crop = 'images/style/wave_crop.jpg'\n",
    "tiger = 'images/style/tiger.jpg'\n",
    "small_world = 'images/style/small_world.jpg'\n",
    "composition_x = 'images/style/composition_x.jpg'\n",
    "sky = 'images/style/sky.jpg'\n",
    "\n",
    "bridge_image = preprocess_image(test_content_image_1)\n",
    "lucechapel_image = preprocess_image(lucechapel)\n",
    "handsome_image = preprocess_image(handsome)\n",
    "cat_image = preprocess_image(cat)\n",
    "bike_image = preprocess_image(bike)\n",
    "uchiha_image = preprocess_image(uchiha)\n",
    "autumn_image = preprocess_image(autumn)\n",
    "street_image = preprocess_image(street)\n",
    "\n",
    "\n",
    "snow_image = preprocess_image(snow)\n",
    "starry_night_image = preprocess_image(starry_night)\n",
    "crystal_image = preprocess_image(crystal)\n",
    "la_muse_image = preprocess_image(la_muse)\n",
    "udnie_image = preprocess_image(udnie)\n",
    "water_image = preprocess_image(water)\n",
    "night_image = preprocess_image(night)\n",
    "chingmin_image = preprocess_image(chingmin)\n",
    "colorhole_image = preprocess_image(colorhole)\n",
    "des_glaneuses_image = preprocess_image(des_glaneuses)\n",
    "jingdo_image = preprocess_image(jingdo)\n",
    "monalisa_image = preprocess_image(monalisa)\n",
    "mountainwater_image = preprocess_image(mountainwater)\n",
    "picassoself_image = preprocess_image(picassoself)\n",
    "wave_crop_image = preprocess_image(wave_crop)\n",
    "tiger_image = preprocess_image(tiger)\n",
    "small_world_image = preprocess_image(small_world)\n",
    "composition_x_image = preprocess_image(composition_x)\n",
    "sky_image = preprocess_image(sky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wct_style_swap(K.concatenate([K.variable(creepy), K.variable(starry_night)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#benben_swap_layer(K.concatenate([K.variable(creepy), K.variable(starry_night)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#encode_net = build_encode_net_vgg16_3_1((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "encode_net = build_encode_net_with_swap_3_1((IMG_WIDTH, IMG_HEIGHT, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_net_2 = build_encode_net_with_swap_3_3((IMG_WIDTH, IMG_HEIGHT, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "content_input (InputLayer)      (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "style_input (InputLayer)        (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512, 512, 3)  0           content_input[0][0]              \n",
      "                                                                 style_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 512, 512, 64) 1792        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 512, 512, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 256, 256, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 256, 256, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 256, 256, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 128, 128, 128 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 128, 128, 256 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64, 64, 256)  0           block3_conv1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 555,328\n",
      "Trainable params: 0\n",
      "Non-trainable params: 555,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encode_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverse_net = InverseNet_3_1((int(IMG_WIDTH/4) ,int(IMG_HEIGHT/4) ,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inverse_net.load_weights(\"models/inverse_net_vgg16.h5\", by_name=True)\n",
    "inverse_net.load_weights(\"models/inverse_net_vgg19.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "swapped_input (InputLayer)   (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 256)     590080    \n",
      "_________________________________________________________________\n",
      "instance_normalization_1 (In (None, 128, 128, 256)     2         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 256, 256, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 128)     295040    \n",
      "_________________________________________________________________\n",
      "instance_normalization_2 (In (None, 256, 256, 128)     2         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 256, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "instance_normalization_3 (In (None, 256, 256, 128)     2         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 512, 512, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 512, 512, 64)      73792     \n",
      "_________________________________________________________________\n",
      "instance_normalization_4 (In (None, 512, 512, 64)      2         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512, 512, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 512, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "instance_normalization_5 (In (None, 512, 512, 64)      2         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512, 512, 64)      0         \n",
      "_________________________________________________________________\n",
      "inverse_net_output (Conv2D)  (None, 512, 512, 3)       1731      \n",
      "=================================================================\n",
      "Total params: 1,145,165\n",
      "Trainable params: 1,145,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inverse_net.compile(optimizer=\"adam\", loss='mse')\n",
    "inverse_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# bridge_image = preprocess_image(test_content_image_1)\n",
    "# lucechapel_image = preprocess_image(lucechapel)\n",
    "# handsome_image = preprocess_image(handsome)\n",
    "# cat_image\n",
    "\n",
    "# snow_image = preprocess_image(snow)\n",
    "# starry_night_image = preprocess_image(starry_night)\n",
    "# crystal_image = preprocess_image(crystal)\n",
    "# la_muse_image = preprocess_image(la_muse)\n",
    "# udnie_image = preprocess_image(udnie)\n",
    "# water_image = preprocess_image(water)\n",
    "# night_image = preprocess_image(night)\n",
    "# chingmin_image = preprocess_image(chingmin)\n",
    "# colorhole_image = preprocess_image(colorhole)\n",
    "# des_glaneuses_image = preprocess_image(des_glaneuses)\n",
    "# jingdo_image = preprocess_image(jingdo)\n",
    "# monalisa_image = preprocess_image(monalisa)\n",
    "# mountainwater_image = preprocess_image(mountainwater)\n",
    "# picassoself_image = preprocess_image(picassoself)\n",
    "# wave_crop_image = preprocess_image(wave_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_images(content, content_name, save=False):\n",
    "    features = []\n",
    "    styles = [snow_image, starry_night_image, crystal_image, la_muse_image, udnie_image, water_image, night_image, \n",
    "             chingmin_image, colorhole_image, des_glaneuses_image, jingdo_image, monalisa_image, mountainwater_image, picassoself_image\n",
    "             , wave_crop_image, tiger_image, small_world_image, composition_x_image, sky_image]\n",
    "    \n",
    "    style_names = [\"snow\", \"starry_night\", \"crystal\", \"la_muse\", \"udnie\", \"water\", \"night\", \"chingmin\", \"colorhole\", \"des_glaneuses\",\n",
    "                  \"jingdo\", \"monalisa\", \"mountainwater\", \"picassoself\", \"wave_crop\", \"tiger\", \"small_world\", \"composition_x\"\n",
    "                  , \"sky\"]\n",
    "    \n",
    "    for s in styles:\n",
    "        features.append(encode_net_2.predict([content, s]))\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        \n",
    "        show_without_deprocess(x=inverse_net.predict([features[i]]), save=save, name=content_name+\"_\"+style_names[i])\n",
    "        \n",
    "def transform_iterate_images(content_name, iterations=3, iterate=0, save=False):\n",
    "    features = []\n",
    "    styles = [snow_image, starry_night_image, crystal_image, la_muse_image, udnie_image, water_image, night_image, \n",
    "             chingmin_image, colorhole_image, des_glaneuses_image, jingdo_image, monalisa_image, mountainwater_image, picassoself_image\n",
    "             , wave_crop_image, tiger_image, small_world_image, composition_x_image, sky_image]\n",
    "    \n",
    "    style_names = [\"snow\", \"starry_night\", \"crystal\", \"la_muse\", \"udnie\", \"water\", \"night\", \"chingmin\", \"colorhole\", \"des_glaneuses\",\n",
    "                  \"jingdo\", \"monalisa\", \"mountainwater\", \"picassoself\", \"wave_crop\", \"tiger\", \"small_world\", \"composition_x\"\n",
    "                  , \"sky\"]\n",
    "    \n",
    "    \n",
    "    for s in range(len(styles)):\n",
    "        content = preprocess_image(\"output/%s_%s_%d.jpg\" % (content_name, style_names[s], iterate))\n",
    "        features.append(encode_net_2.predict([content, styles[s]]))\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        show_without_deprocess(x=inverse_net.predict([features[i]]), save=save, name=content_name+\"_\"+style_names[i], iterate=iterate+1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_iterate1 = 'output/101_iterate2.jpg'\n",
    "# test_iterate2 = 'output/cat_iterate2.jpg'\n",
    "# test_iterate1_img = preprocess_image(test_iterate1)\n",
    "# test_iterate2_img = preprocess_image(test_iterate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image1_feature = encode_net.predict([bridge_image, snow_image])\n",
    "image2_feature = encode_net.predict([bridge_image, starry_night_image])\n",
    "image3_feature = encode_net.predict([bridge_image, crystal_image])\n",
    "image4_feature = encode_net.predict([bridge_image, la_muse_image])\n",
    "image5_feature = encode_net.predict([bridge_image, udnie_image])\n",
    "image6_feature = encode_net.predict([bridge_image, water_image])\n",
    "\n",
    "# image1_feature = encode_net_2.predict([test_iterate1_img, water_image])\n",
    "# image2_feature = encode_net_2.predict([test_iterate2_img, water_image])\n",
    "# image3_feature = encode_net_2.predict([handsome_image, uchiha_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_1 = inverse_net.predict([image1_feature])\n",
    "predict_2 = inverse_net.predict([image2_feature])\n",
    "predict_3 = inverse_net.predict([image3_feature])\n",
    "predict_4 = inverse_net.predict([image4_feature])\n",
    "predict_5 = inverse_net.predict([image5_feature])\n",
    "predict_6 = inverse_net.predict([image6_feature])\n",
    "\n",
    "# predict_1 = inverse_net.predict([image1_feature])\n",
    "# predict_2 = inverse_net.predict([image2_feature])\n",
    "# predict_3 = inverse_net.predict([image3_feature])\n",
    "# show_without_deprocess(predict_1[0], save=True, name='test2_iterate1_img')\n",
    "# show_without_deprocess(predict_2[0], save=True, name='test2_iterate2_img')\n",
    "# show_without_deprocess(predict_3[0], save=True, name='test2_iterate3_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 512, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_net_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-cec927d12d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtransform_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreet_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"street\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-c5e7cd91d219>\u001b[0m in \u001b[0;36mtransform_images\u001b[1;34m(content, content_name, save)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstyles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode_net_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encode_net_2' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "transform_images(street_image, \"street\", True)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_iterate_images(\"street\", save=True, iterate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print((end - start) / 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling Image to (512, 512)\n",
      "[[[148 126 131]\n",
      "  [197 179 187]\n",
      "  [178 171 171]\n",
      "  ...\n",
      "  [157 157 182]\n",
      "  [172 167 185]\n",
      "  [189 181 185]]\n",
      "\n",
      " [[167 139 148]\n",
      "  [188 173 183]\n",
      "  [168 171 171]\n",
      "  ...\n",
      "  [147 156 196]\n",
      "  [137 129 168]\n",
      "  [190 176 191]]\n",
      "\n",
      " [[183 152 153]\n",
      "  [189 173 168]\n",
      "  [176 181 164]\n",
      "  ...\n",
      "  [165 181 195]\n",
      "  [162 154 180]\n",
      "  [187 169 179]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[185 163 170]\n",
      "  [181 178 187]\n",
      "  [179 195 188]\n",
      "  ...\n",
      "  [147 165 155]\n",
      "  [164 153 151]\n",
      "  [192 169 166]]\n",
      "\n",
      " [[202 168 164]\n",
      "  [216 177 186]\n",
      "  [205 175 181]\n",
      "  ...\n",
      "  [159 156 163]\n",
      "  [180 145 157]\n",
      "  [222 183 185]]\n",
      "\n",
      " [[183 149 141]\n",
      "  [204 154 160]\n",
      "  [183 133 142]\n",
      "  ...\n",
      "  [135 110 124]\n",
      "  [142  92 108]\n",
      "  [159 117 115]]]\n",
      "Rescaling Image to (512, 512)\n",
      "[[[181 146 136]\n",
      "  [187 137 142]\n",
      "  [189 157 159]\n",
      "  ...\n",
      "  [174 160 147]\n",
      "  [181 156 138]\n",
      "  [173 152 137]]\n",
      "\n",
      " [[176 131 139]\n",
      "  [117  78 100]\n",
      "  [126 113 135]\n",
      "  ...\n",
      "  [133 125 132]\n",
      "  [158 131 124]\n",
      "  [190 165 147]]\n",
      "\n",
      " [[190 171 183]\n",
      "  [148 155 184]\n",
      "  [133 171 198]\n",
      "  ...\n",
      "  [111 127 148]\n",
      "  [143 136 136]\n",
      "  [191 175 157]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[208 162  96]\n",
      "  [200 169  38]\n",
      "  [204 190  19]\n",
      "  ...\n",
      "  [ 84 145 133]\n",
      "  [ 90 104  93]\n",
      "  [106 104  88]]\n",
      "\n",
      " [[205 163 116]\n",
      "  [208 177  82]\n",
      "  [195 175  45]\n",
      "  ...\n",
      "  [127 133  95]\n",
      "  [162 131  96]\n",
      "  [187 163 132]]\n",
      "\n",
      " [[171 134 115]\n",
      "  [189 150 107]\n",
      "  [155 118  54]\n",
      "  ...\n",
      "  [169 139 105]\n",
      "  [215 167 126]\n",
      "  [190 157 119]]]\n",
      "Rescaling Image to (512, 512)\n",
      "[[[140 133 126]\n",
      "  [152 160 160]\n",
      "  [126 156 155]\n",
      "  ...\n",
      "  [211 178 164]\n",
      "  [202 176 180]\n",
      "  [217 200 204]]\n",
      "\n",
      " [[148 148 138]\n",
      "  [118 157 149]\n",
      "  [ 97 172 164]\n",
      "  ...\n",
      "  [209 179 151]\n",
      "  [159 123 140]\n",
      "  [202 170 189]]\n",
      "\n",
      " [[153 158 137]\n",
      "  [120 168 142]\n",
      "  [104 189 158]\n",
      "  ...\n",
      "  [213 202 141]\n",
      "  [189 158 150]\n",
      "  [210 176 183]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[172 142 151]\n",
      "  [161 137 156]\n",
      "  [156 154 153]\n",
      "  ...\n",
      "  [225 212 138]\n",
      "  [222 186 146]\n",
      "  [221 174 161]]\n",
      "\n",
      " [[179 150 163]\n",
      "  [165 123 163]\n",
      "  [171 127 155]\n",
      "  ...\n",
      "  [249 204 142]\n",
      "  [255 199 166]\n",
      "  [254 194 183]]\n",
      "\n",
      " [[156 126 137]\n",
      "  [157 108 142]\n",
      "  [167 109 138]\n",
      "  ...\n",
      "  [249 184 153]\n",
      "  [255 179 163]\n",
      "  [206 148 135]]]\n",
      "Rescaling Image to (512, 512)\n",
      "[[[127  96 102]\n",
      "  [168 132 141]\n",
      "  [118  80  87]\n",
      "  ...\n",
      "  [115  74  89]\n",
      "  [159 141 154]\n",
      "  [176 175 171]]\n",
      "\n",
      " [[148 106 117]\n",
      "  [179 139 151]\n",
      "  [ 99  54  70]\n",
      "  ...\n",
      "  [ 88  23  56]\n",
      "  [146 122 155]\n",
      "  [215 212 213]]\n",
      "\n",
      " [[165 124 116]\n",
      "  [215 178 162]\n",
      "  [150  99  95]\n",
      "  ...\n",
      "  [107  34  52]\n",
      "  [151 125 149]\n",
      "  [206 201 200]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[109 108  76]\n",
      "  [148 166  81]\n",
      "  [187 193  43]\n",
      "  ...\n",
      "  [149 135  43]\n",
      "  [ 79  57  29]\n",
      "  [ 50  39  27]]\n",
      "\n",
      " [[148 136 124]\n",
      "  [190 189 139]\n",
      "  [182 170  64]\n",
      "  ...\n",
      "  [ 77  61  38]\n",
      "  [ 39  25  46]\n",
      "  [ 62  64  72]]\n",
      "\n",
      " [[158 136 138]\n",
      "  [195 170 160]\n",
      "  [172 137  94]\n",
      "  ...\n",
      "  [ 74  49  60]\n",
      "  [100  82 104]\n",
      "  [109 106 107]]]\n",
      "Rescaling Image to (512, 512)\n",
      "[[[123 104  99]\n",
      "  [122 108 106]\n",
      "  [101  95  90]\n",
      "  ...\n",
      "  [163 145 104]\n",
      "  [162 141 110]\n",
      "  [137 118 105]]\n",
      "\n",
      " [[115  86  93]\n",
      "  [ 71  54  70]\n",
      "  [ 55  51  62]\n",
      "  ...\n",
      "  [154 145  84]\n",
      "  [149 138  85]\n",
      "  [156 140 114]]\n",
      "\n",
      " [[ 81  48  55]\n",
      "  [ 48  29  44]\n",
      "  [ 65  62  67]\n",
      "  ...\n",
      "  [157 154 103]\n",
      "  [148 142  84]\n",
      "  [153 142 106]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[122  89  91]\n",
      "  [ 81  52  63]\n",
      "  [102  87  87]\n",
      "  ...\n",
      "  [ 98  89  77]\n",
      "  [133 109  93]\n",
      "  [166 145 122]]\n",
      "\n",
      " [[129  94  92]\n",
      "  [ 94  58  65]\n",
      "  [116  90  88]\n",
      "  ...\n",
      "  [110  93  93]\n",
      "  [148 110 110]\n",
      "  [191 159 147]]\n",
      "\n",
      " [[160 129 122]\n",
      "  [184 150 144]\n",
      "  [198 169 155]\n",
      "  ...\n",
      "  [172 135 144]\n",
      "  [177 125 132]\n",
      "  [168 127 119]]]\n",
      "Rescaling Image to (512, 512)\n",
      "[[[127 108 107]\n",
      "  [115  92  99]\n",
      "  [ 96  78  81]\n",
      "  ...\n",
      "  [120 121 127]\n",
      "  [153 146 142]\n",
      "  [157 150 140]]\n",
      "\n",
      " [[124  90  95]\n",
      "  [ 76  48  57]\n",
      "  [ 79  61  69]\n",
      "  ...\n",
      "  [ 70  71 107]\n",
      "  [104  89 111]\n",
      "  [151 138 138]]\n",
      "\n",
      " [[131  96  88]\n",
      "  [103  79  71]\n",
      "  [ 98  96  81]\n",
      "  ...\n",
      "  [ 48  57 100]\n",
      "  [ 46  28  66]\n",
      "  [113 100 105]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[133 115 130]\n",
      "  [ 92  84 124]\n",
      "  [120 139 162]\n",
      "  ...\n",
      "  [100 121 146]\n",
      "  [ 91  85 118]\n",
      "  [ 97  85  96]]\n",
      "\n",
      " [[152 131 134]\n",
      "  [100  86 118]\n",
      "  [103 116 148]\n",
      "  ...\n",
      "  [ 88  93 129]\n",
      "  [ 88  65 105]\n",
      "  [104  83  95]]\n",
      "\n",
      " [[155 133 127]\n",
      "  [140 124 133]\n",
      "  [122 124 138]\n",
      "  ...\n",
      "  [127 107 141]\n",
      "  [138 100 130]\n",
      "  [151 122 125]]]\n"
     ]
    }
   ],
   "source": [
    "show_without_deprocess(predict_1[0], save=True, name='101_snow_image')\n",
    "show_without_deprocess(predict_2[0], save=True, name='101_starry_night_image')\n",
    "show_without_deprocess(predict_3[0], save=True, name='101_crystal_image')\n",
    "show_without_deprocess(predict_4[0], save=True, name='101_la_muse_image')\n",
    "show_without_deprocess(predict_5[0], save=True, name='101_udnie_image')\n",
    "show_without_deprocess(predict_6[0], save=True, name='101_water_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_without_deprocess(predict_3[0], save=False, name='cat_water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
