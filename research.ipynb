{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5989edea231b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msymbol_database\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_symbol_database\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# @@protoc_insertion_point(imports)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\google\\protobuf\\symbol_database.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor_pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmessage_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\google\\protobuf\\descriptor_pool.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor_database\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Henry\\Anaconda3\\envs\\keras-gpu\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.misc import imresize, imsave, fromimage, toimage\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import warnings\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import img_util\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D,Deconvolution2D \n",
    "from keras.layers.convolutional import Conv2D,UpSampling2D,Cropping2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.layers.core import Activation\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import Regularizer\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 400\n",
    "IMG_WIDTH = 400\n",
    "chosen_model = 'vgg16'\n",
    "# Need to pass image path\n",
    "base_image_path = 'static\\\\img\\\\101.jpg'\n",
    "style_image_path = 'static\\\\img\\\\des_glaneuses.jpg'\n",
    "\n",
    "content_weight = 0.025\n",
    "style_weight = 1.0\n",
    "total_variation_weight = 8.5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of the generated picture.\n",
    "def get_ratio(image):\n",
    "    img = np.asarray(Image.open(image).convert('RGB')).astype('float')\n",
    "    img_WIDTH = img.shape[0]\n",
    "    img_HEIGHT = img.shape[1]\n",
    "    aspect_ratio = float(img_HEIGHT) / img_WIDTH\n",
    "    return aspect_ratio\n",
    "\n",
    "aspect_ratio = get_ratio(base_image_path)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    mode = \"RGB\"\n",
    "    img = imread(image_path, pilmode=mode)\n",
    "\n",
    "    img = resize(img,(IMG_WIDTH, IMG_HEIGHT),anti_aliasing=True)\n",
    "    \n",
    "    # Convert the image to a 0-255 scale\n",
    "    img = 255 * img\n",
    "    # RGB -> BGR\n",
    "#     img = img[:, :, ::-1]\n",
    "\n",
    "#     img[:, :, 0] -= 103.939\n",
    "#     img[:, :, 1] -= 116.779\n",
    "#     img[:, :, 2] -= 123.68\n",
    "\n",
    "    # This function do the same thing above\n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    # Theano's image channel format -- (channels, rows, cols)\n",
    "    # Tensorflow's image channel format -- (rows, cols, channels)\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        img = img.transpose((2, 0, 1)).astype('float32')\n",
    "\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    print(img.shape)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        x = x.reshape((3, IMG_WIDTH, IMG_HEIGHT))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "\n",
    "    # BGR -> RGB\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def pooling_func(x, pooltype=0):\n",
    "    if pooltype == 1:\n",
    "        return AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "    else:\n",
    "        return MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_1 = preprocess_image(base_image_path)\n",
    "TEST_1 = deprocess_image(TEST_1)\n",
    "plt.imshow(TEST_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_image = K.variable(preprocess_image(style_image_path))\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    combination_image = K.placeholder((1, 3, IMG_WIDTH, IMG_HEIGHT))\n",
    "else:\n",
    "    combination_image = K.placeholder((1, IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "image_tensors = [base_image, style_image, combination_image]\n",
    "nb_tensors = len(image_tensors)\n",
    "nb_style_images = nb_tensors - 2 # Content and Output image not considered\n",
    "\n",
    "input_tensor = K.concatenate(image_tensors, axis=0)\n",
    "\n",
    "if K.image_dim_ordering() == \"th\":\n",
    "    input_shape = (3, IMG_WIDTH, IMG_HEIGHT,None)\n",
    "else:\n",
    "    input_shape = (None, IMG_WIDTH, IMG_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(tensor=input_tensor, batch_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputNormalize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InputNormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #x = (x - 127.5)/ 127.5\n",
    "        return x/255.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv_bn_relu(nb_filter, nb_row, nb_col,stride):   \n",
    "    def conv_func(x):\n",
    "        x = Conv2D(nb_filter, (nb_row, nb_col), strides=stride,padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        #x = LeakyReLU(0.2)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "    return conv_func\n",
    "\n",
    "\n",
    "\n",
    "#https://keunwoochoi.wordpress.com/2016/03/09/residual-networks-implementation-on-keras/\n",
    "def res_conv(nb_filter, nb_row, nb_col,stride=(1,1)):\n",
    "    def _res_func(x):\n",
    "        identity = Cropping2D(cropping=((2,2),(2,2)))(x)\n",
    "\n",
    "        a = Conv2D(nb_filter, (nb_row, nb_col), strides=stride, padding='valid')(x)\n",
    "        a = BatchNormalization()(a)\n",
    "        #a = LeakyReLU(0.2)(a)\n",
    "        a = Activation(\"relu\")(a)\n",
    "        a = Conv2D(nb_filter, (nb_row, nb_col), strides=stride, padding='valid')(a)\n",
    "        y = BatchNormalization()(a)\n",
    "\n",
    "        return  add([identity, y])\n",
    "\n",
    "    return _res_func\n",
    "\n",
    "\n",
    "def dconv_bn_nolinear(nb_filter, nb_row, nb_col,stride=(2,2),activation=\"relu\"):\n",
    "    def _dconv_bn(x):\n",
    "        #TODO: Deconvolution2D\n",
    "        #x = Deconvolution2D(nb_filter,nb_row, nb_col, output_shape=output_shape, subsample=stride, border_mode='same')(x)\n",
    "        #x = UpSampling2D(size=stride)(x)\n",
    "        x = UnPooling2D(size=stride)(x)\n",
    "        x = ReflectionPadding2D(padding=stride)(x)\n",
    "        x = Conv2D(nb_filter, (nb_row, nb_col), padding='valid')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation)(x)\n",
    "        return x\n",
    "    return _dconv_bn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Denormalize(Layer):\n",
    "    '''\n",
    "    Custom layer to denormalize the final Convolution layer activations (tanh)\n",
    "    Since tanh scales the output to the range (-1, 1), we add 1 to bring it to the\n",
    "    range (0, 2). We then multiply it by 127.5 to scale the values to the range (0, 255)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Denormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "        Scales the tanh output activations from previous layer (-1, 1) to the\n",
    "        range (0, 255)\n",
    "        '''\n",
    "\n",
    "        return (x + 1) * 127.5\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class VGGNormalize(Layer):\n",
    "    '''\n",
    "    Custom layer to subtract the outputs of previous layer by 120,\n",
    "    to normalize the inputs to the VGG network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VGGNormalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # No exact substitute for set_subtensor in tensorflow\n",
    "        # So we subtract an approximate value       \n",
    "        \n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, :, :, ::-1]       \n",
    "        x -= 120\n",
    "        #img_util.preprocess_image(style_image_path, img_width, img_height)\n",
    "        return x\n",
    "   \n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), dim_ordering='default', **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        self.padding = padding\n",
    "        if isinstance(padding, dict):\n",
    "            if set(padding.keys()) <= {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}:\n",
    "                self.top_pad = padding.get('top_pad', 0)\n",
    "                self.bottom_pad = padding.get('bottom_pad', 0)\n",
    "                self.left_pad = padding.get('left_pad', 0)\n",
    "                self.right_pad = padding.get('right_pad', 0)\n",
    "            else:\n",
    "                raise ValueError('Unexpected key found in `padding` dictionary. '\n",
    "                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", '\n",
    "                                 '\"left_pad\", \"right_pad\"}.'\n",
    "                                 'Found: ' + str(padding.keys()))\n",
    "        else:\n",
    "            padding = tuple(padding)\n",
    "            if len(padding) == 2:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[0]\n",
    "                self.left_pad = padding[1]\n",
    "                self.right_pad = padding[1]\n",
    "            elif len(padding) == 4:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[1]\n",
    "                self.left_pad = padding[2]\n",
    "                self.right_pad = padding[3]\n",
    "            else:\n",
    "                raise TypeError('`padding` should be tuple of int '\n",
    "                                'of length 2 or 4, or dict. '\n",
    "                                'Found: ' + str(padding))\n",
    "\n",
    "        if dim_ordering not in {'tf'}:\n",
    "            raise ValueError('dim_ordering must be in {tf}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=4)] \n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        top_pad=self.top_pad\n",
    "        bottom_pad=self.bottom_pad\n",
    "        left_pad=self.left_pad\n",
    "        right_pad=self.right_pad        \n",
    "        \n",
    "        paddings = [[0,0],[left_pad,right_pad],[top_pad,bottom_pad],[0,0]]\n",
    "\n",
    "        \n",
    "        return tf.pad(x,paddings, mode='REFLECT', name=None)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        if self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1] + self.top_pad + self.bottom_pad if input_shape[1] is not None else None\n",
    "            cols = input_shape[2] + self.left_pad + self.right_pad if input_shape[2] is not None else None\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding}\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))     \n",
    "    \n",
    "    \n",
    "class UnPooling2D(UpSampling2D):\n",
    "    def __init__(self, size=(2, 2)):\n",
    "        super(UnPooling2D, self).__init__(size)\n",
    "\n",
    "  \n",
    "    def call(self, x, mask=None):\n",
    "        shapes = x.get_shape().as_list() \n",
    "        w = self.size[0] * shapes[1]\n",
    "        h = self.size[1] * shapes[2]\n",
    "        return tf.image.resize_nearest_neighbor(x, (w,h))\n",
    "\n",
    "        \n",
    "\n",
    "class InstanceNormalize(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InstanceNormalize, self).__init__(**kwargs)\n",
    "        self.epsilon = 1e-3\n",
    "            \n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mean, var = tf.nn.moments(x, [1, 2], keep_dims=True)\n",
    "        return tf.div(tf.subtract(x, mean), tf.sqrt(tf.add(var, self.epsilon)))\n",
    "\n",
    "                                                 \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "\n",
    "    shape = K.shape(x)\n",
    "    \n",
    "    C, W, H = (shape[0],shape[1], shape[2])\n",
    "    \n",
    "    cf = K.reshape(features ,(C,-1))\n",
    "    gram = K.dot(cf, K.transpose(cf)) /  K.cast(C*W*H,dtype='float32')\n",
    "\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "        assert K.ndim(style) == 3\n",
    "        assert K.ndim(combination) == 3\n",
    "\n",
    "        S = gram_matrix(style)\n",
    "        C = gram_matrix(combination)\n",
    "        channels = 3\n",
    "        size = IMG_WIDTH * IMG_HEIGHT\n",
    "        return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "\n",
    "# an auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "def content_loss(base, combination):\n",
    "\n",
    "    return 0.5 * K.sum(K.square(content_weight * (combination - base)))\n",
    "\n",
    "# the 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        a = K.square(x[:, :, :IMG_WIDTH - 1, :IMG_HEIGHT - 1] - x[:, :, 1:, :IMG_HEIGHT - 1])\n",
    "        b = K.square(x[:, :, :IMG_WIDTH - 1, :IMG_HEIGHT - 1] - x[:, :, :IMG_WIDTH - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :IMG_WIDTH - 1, :IMG_HEIGHT - 1, :] - x[:, 1:, :IMG_HEIGHT - 1, :])\n",
    "        b = K.square(x[:, :IMG_WIDTH - 1, :IMG_HEIGHT - 1, :] - x[:, :IMG_WIDTH - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "class StyleReconstructionRegularizer(Regularizer):\n",
    "    \"\"\" Johnson et al 2015 https://arxiv.org/abs/1603.08155 \"\"\"\n",
    "\n",
    "    def __init__(self, style_feature_target, weight=1.0):\n",
    "        self.style_feature_target = style_feature_target\n",
    "        self.weight = weight\n",
    "        self.uses_learning_phase = False\n",
    "        super(StyleReconstructionRegularizer, self).__init__()\n",
    "\n",
    "        self.style_gram = gram_matrix(style_feature_target)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        output = x.output[0] # Generated by network\n",
    "        loss = self.weight *  K.sum(K.mean(K.square((self.style_gram-gram_matrix(output) )))) \n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FeatureReconstructionRegularizer(Regularizer):\n",
    "    \"\"\" Johnson et al 2015 https://arxiv.org/abs/1603.08155 \"\"\"\n",
    "\n",
    "    def __init__(self, weight=1.0):\n",
    "        self.weight = weight\n",
    "        self.uses_learning_phase = False\n",
    "        super(FeatureReconstructionRegularizer, self).__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        generated = x.output[0] # Generated by network features\n",
    "        content = x.output[1] # True X input features\n",
    "\n",
    "        loss = self.weight *  K.sum(K.mean(K.square(content-generated)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TVRegularizer(Regularizer):\n",
    "    \"\"\" Enforces smoothness in image output. \"\"\"\n",
    "\n",
    "    def __init__(self, weight=1.0):\n",
    "        self.weight = weight\n",
    "        self.uses_learning_phase = False\n",
    "        super(TVRegularizer, self).__init__()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        assert K.ndim(x.output) == 4\n",
    "        x_out = x.output\n",
    "        \n",
    "        shape = K.shape(x_out)\n",
    "        img_width, img_height,channel = (shape[1],shape[2], shape[3])\n",
    "        size = img_width * img_height * channel \n",
    "        if K.image_dim_ordering() == 'th':\n",
    "            a = K.square(x_out[:, :, :img_width - 1, :img_height - 1] - x_out[:, :, 1:, :img_height - 1])\n",
    "            b = K.square(x_out[:, :, :img_width - 1, :img_height - 1] - x_out[:, :, :img_width - 1, 1:])\n",
    "        else:\n",
    "            a = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, 1:, :img_height - 1, :])\n",
    "            b = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, :img_width - 1, 1:, :])\n",
    "        loss = self.weight * K.sum(K.pow(a + b, 1.25)) \n",
    "        return loss\n",
    "    \n",
    "def add_style_loss(vgg,style_image_path,vgg_layers,vgg_output_dict,img_width, img_height,weight):\n",
    "    style_img = img_util.preprocess_image(style_image_path, img_width, img_height)\n",
    "    print('Getting style features from VGG network.')\n",
    "\n",
    "    style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3']\n",
    "\n",
    "    style_layer_outputs = []\n",
    "\n",
    "    for layer in style_layers:\n",
    "        style_layer_outputs.append(vgg_output_dict[layer])\n",
    "\n",
    "    vgg_style_func = K.function([vgg.layers[-19].input], style_layer_outputs)\n",
    "\n",
    "    style_features = vgg_style_func([style_img])\n",
    "\n",
    "    # Style Reconstruction Loss\n",
    "    for i, layer_name in enumerate(style_layers):\n",
    "        layer = vgg_layers[layer_name]\n",
    "\n",
    "        feature_var = K.variable(value=style_features[i][0])\n",
    "        style_loss = StyleReconstructionRegularizer(\n",
    "                            style_feature_target=feature_var,\n",
    "                            weight=weight)(layer)\n",
    "\n",
    "        layer.add_loss(style_loss)\n",
    "\n",
    "def add_content_loss(vgg_layers,vgg_output_dict,weight):\n",
    "    # Feature Reconstruction Loss\n",
    "    content_layer = 'block3_conv3'\n",
    "    content_layer_output = vgg_output_dict[content_layer]\n",
    "\n",
    "    layer = vgg_layers[content_layer]\n",
    "    content_regularizer = FeatureReconstructionRegularizer(weight)(layer)\n",
    "    layer.add_loss(content_regularizer)\n",
    "\n",
    "\n",
    "def add_total_variation_loss(transform_output_layer,weight):\n",
    "    # Total Variation Regularization\n",
    "    layer = transform_output_layer  # Output layer\n",
    "    tv_regularizer = TVRegularizer(weight)(layer)\n",
    "    layer.add_loss(tv_regularizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_transform_net(img_width,img_height,tv_weight=1):\n",
    "    x = Input(shape=(img_width,img_height,3))\n",
    "    a = InputNormalize()(x)\n",
    "    a = ReflectionPadding2D(padding=(40,40),input_shape=(img_width,img_height,3))(a)\n",
    "    a = conv_bn_relu(32, 9, 9, stride=(1,1))(a)\n",
    "    a = conv_bn_relu(64, 9, 9, stride=(2,2))(a)\n",
    "    a = conv_bn_relu(128, 3, 3, stride=(2,2))(a)\n",
    "    for i in range(5):\n",
    "        a = res_conv(128,3,3)(a)\n",
    "    a = dconv_bn_nolinear(64,3,3)(a)\n",
    "    a = dconv_bn_nolinear(32,3,3)(a)\n",
    "    a = dconv_bn_nolinear(3,9,9,stride=(1,1),activation=\"tanh\")(a)\n",
    "    # Scale output to range [0, 255] via custom Denormalize layer\n",
    "    y = Denormalize(name='transform_output')(a)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    \n",
    "#     if tv_weight > 0:\n",
    "#         add_total_variation_loss(model.layers[-1],tv_weight)\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform_model = image_transform_net(IMG_WIDTH, IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting style features from VGG network.\n"
     ]
    }
   ],
   "source": [
    "def vgg_loss_net(x_in, trux_x_in, width, height, style_image_path,content_weight,style_weight):\n",
    "    # Append the initial input to the FastNet input to the VGG inputs\n",
    "    x = concatenate([x_in, trux_x_in], axis=0)\n",
    "    \n",
    "    # Normalize the inputs via custom VGG Normalization layer\n",
    "    x = VGGNormalize(name=\"vgg_normalize\")(x)\n",
    "\n",
    "    vgg = VGG16(include_top=False,input_tensor=x)\n",
    "\n",
    "    vgg_output_dict = dict([(layer.name, layer.output) for layer in vgg.layers[-18:]])\n",
    "    vgg_layers = dict([(layer.name, layer) for layer in vgg.layers[-18:]])\n",
    "\n",
    "    if style_weight > 0:\n",
    "        add_style_loss(vgg,style_image_path , vgg_layers, vgg_output_dict, width, height,style_weight)   \n",
    "\n",
    "    if content_weight > 0:\n",
    "        add_content_loss(vgg_layers,vgg_output_dict,content_weight)\n",
    "\n",
    "    # Freeze all VGG layers\n",
    "    for layer in vgg.layers[-19:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return vgg\n",
    "\n",
    "vgg16_model = vgg_loss_net(img_transform_model.output,img_transform_model.input,IMG_WIDTH, IMG_HEIGHT, style_image_path, content_weight, style_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 400, 400, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_normalize_1 (InputNormali (None, 400, 400, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_1 (Reflect (None, 480, 480, 3)  0           input_normalize_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 480, 480, 32) 7808        reflection_padding2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 480, 480, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 480, 480, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 240, 240, 64) 165952      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 240, 240, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 240, 240, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 120, 120, 128 73856       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 120, 120, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 120, 120, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 118, 118, 128 147584      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 118, 118, 128 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 118, 118, 128 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 116, 116, 128 147584      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 116, 116, 128 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 116, 116, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 116, 116, 128 0           cropping2d_1[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 114, 114, 128 147584      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 114, 114, 128 512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 114, 114, 128 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 112, 112, 128 147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 112, 112, 128 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 112, 112, 128 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 112, 112, 128 0           cropping2d_2[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 110, 110, 128 147584      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 110, 110, 128 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 110, 110, 128 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 108, 108, 128 147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 108, 108, 128 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 108, 108, 128 512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 108, 108, 128 0           cropping2d_3[0][0]               \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 106, 106, 128 147584      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 106, 106, 128 512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 106, 106, 128 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 104, 104, 128 147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 104, 104, 128 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 104, 104, 128 512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 104, 104, 128 0           cropping2d_4[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 102, 102, 128 147584      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 102, 102, 128 512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 102, 102, 128 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 100, 128 147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_5 (Cropping2D)       (None, 100, 100, 128 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 100, 128 512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 100, 100, 128 0           cropping2d_5[0][0]               \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "un_pooling2d_1 (UnPooling2D)    (None, 200, 200, 128 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_2 (Reflect (None, 204, 204, 128 0           un_pooling2d_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 202, 202, 64) 73792       reflection_padding2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 202, 202, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 202, 202, 64) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "un_pooling2d_2 (UnPooling2D)    (None, 404, 404, 64) 0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_3 (Reflect (None, 408, 408, 64) 0           un_pooling2d_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 406, 406, 32) 18464       reflection_padding2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 406, 406, 32) 128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 406, 406, 32) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "un_pooling2d_3 (UnPooling2D)    (None, 406, 406, 32) 0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reflection_padding2d_4 (Reflect (None, 408, 408, 32) 0           un_pooling2d_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 400, 400, 3)  7779        reflection_padding2d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 400, 400, 3)  12          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 400, 400, 3)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "transform_output (Denormalize)  (None, 400, 400, 3)  0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400, 400, 3)  0           transform_output[0][0]           \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "vgg_normalize (VGGNormalize)    (None, 400, 400, 3)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 400, 400, 64) 1792        vgg_normalize[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 400, 400, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 200, 200, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 200, 200, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 200, 200, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 100, 100, 128 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 100, 100, 256 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 100, 100, 256 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 100, 100, 256 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 50, 50, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 50, 50, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 50, 50, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 50, 50, 512)  2359808     block4_conv2[0][0]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 25, 25, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 25, 25, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 25, 25, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 25, 25, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 12, 12, 512)  0           block5_conv3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 16,544,591\n",
      "Trainable params: 1,826,697\n",
      "Non-trainable params: 14,717,894\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in vgg16_model.layers])\n",
    "shape_dict = dict([(layer.name, layer.output_shape) for layer in vgg16_model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chosen_model == \"vgg19\":\n",
    "    feature_layers = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3', 'conv3_4',\n",
    "                          'conv4_1', 'conv4_2', 'conv4_3', 'conv4_4', 'conv5_1', 'conv5_2', 'conv5_3', 'conv5_4']\n",
    "else:\n",
    "    feature_layers = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3',\n",
    "                          'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2', 'conv5_3']\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool',\n",
    "                     'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', \n",
    "                     'block4_conv3','block4_pool','block5_conv1', 'block5_conv2', 'block5_conv3']\n",
    "\n",
    "\n",
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "print('layer_features : ', layer_features)\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "print('base_image_features : ', base_image_features)\n",
    "combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "print('combination_features : ', combination_features)\n",
    "loss += content_weight * content_loss(base_image_features,\n",
    "                                          combination_features)\n",
    "print('loss : ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement 2\n",
    "# Use all layers for style feature extraction and reconstruction\n",
    "nb_layers = len(feature_layers) - 1\n",
    "\n",
    "channel_index = 1 if K.image_dim_ordering() == \"th\" else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement 3 : Chained Inference without blurring\n",
    "for i in range(len(feature_layers) - 1):\n",
    "    layer_features = outputs_dict[feature_layers[i]]\n",
    "    shape = shape_dict[feature_layers[i]]\n",
    "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
    "\n",
    "    sl1 = []\n",
    "    for j in range(nb_style_images):\n",
    "        sl1.append(style_loss(style_reference_features[j], combination_features))\n",
    "\n",
    "    layer_features = outputs_dict[feature_layers[i + 1]]\n",
    "    shape = shape_dict[feature_layers[i + 1]]\n",
    "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
    "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
    "    sl2 = []\n",
    "    for j in range(nb_style_images):\n",
    "        sl2.append(style_loss(style_reference_features[j], combination_features))\n",
    "\n",
    "    for j in range(nb_style_images):\n",
    "        sl = sl1[j] - sl2[j]\n",
    "\n",
    "        # Improvement 4\n",
    "        # Geometric weighted scaling of style loss\n",
    "        loss += (style_weight / (2 ** (nb_layers - (i + 1)))) * sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "outputs = [loss]\n",
    "if type(grads) in {list, tuple}:\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_outputs = K.function([combination_image], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg16_model.layers:\n",
    "    if layer.name in feature_layers:\n",
    "        layer.add_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[-19:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train part from fast-neural style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_loss(y_true, y_pred ):\n",
    "    return K.variable(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 4000\n",
    "train_batchsize =  1\n",
    "train_image_path = \"static/img/\"\n",
    "\n",
    "learning_rate = 1e-3 #1e-3\n",
    "optimizer = optimizers.Adam() # Adam(lr=learning_rate,beta_1=0.99)\n",
    "\n",
    "vgg16_model.compile(optimizer,  dummy_loss)  # Dummy loss since we are learning from regularizes\n",
    "\n",
    "datagen = image.ImageDataGenerator()\n",
    "\n",
    "dummy_y = np.zeros((train_batchsize,IMG_WIDTH,IMG_HEIGHT,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(x):\n",
    "    img = x\n",
    "\n",
    "#     img_ht = int(IMG_WIDTH * aspect_ratio)\n",
    "#     print(\"Rescaling Image to (%d, %d)\" % (IMG_WIDTH, img_ht))\n",
    "#     img = imresize(img, (IMG_WIDTH, img_ht), interp='bilinear')\n",
    "#     im = toimage(img)\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示圖片\n",
    "def showimg(x):\n",
    "    \n",
    "    # 將張量轉回圖片的後處理\n",
    "    img = deprocess_image(x.copy())\n",
    "    \n",
    "    # 取得原圖比例\n",
    "    aspect_ratio = get_ratio(base_image_path)  \n",
    "    img_ht = int(IMG_WIDTH * aspect_ratio)\n",
    "    print(\"Rescaling Image to (%d, %d)\" % (IMG_WIDTH, img_ht))\n",
    "    img = imresize(img, (IMG_WIDTH, img_ht), interp='bilinear')\n",
    "    im = toimage(img)\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        x = x.reshape((3, IMG_WIDTH, IMG_HEIGHT))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    \n",
    "    # BGR -> RGB\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    # 將陣列的值的範圍縮回 0~255，因為處理的結果有可能出現超過這個範圍的數字\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skip_to = 0\n",
    "style = 'test'\n",
    "i=0\n",
    "t1 = time.time()\n",
    "train_start = time.time()\n",
    "for x in datagen.flow_from_directory(train_image_path, class_mode=None, batch_size=train_batchsize,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT), shuffle=False):    \n",
    "    if i > nb_epoch:\n",
    "        break\n",
    "\n",
    "    if i < skip_to:\n",
    "        i+=train_batchsize\n",
    "        if i % 1000 ==0:\n",
    "            print(\"skip to: %d\" % i)\n",
    "\n",
    "        continue\n",
    "\n",
    "    hist = vgg16_model.train_on_batch(x, dummy_y)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(hist,(time.time() -t1))\n",
    "        t1 = time.time()\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(\"epoc: \", i)\n",
    "        val_x = img_transform_model.predict(x)\n",
    "        \n",
    "#         original_img = deprocess_image(x[0].copy())\n",
    "#         img = deprocess_image(val_x[0].copy())\n",
    "        \n",
    "        display_img(x[0])\n",
    "        display_img(val_x[0])\n",
    "        vgg16_model.save_weights(style+'_weights.h5')\n",
    "\n",
    "    i+=train_batchsize\n",
    "train_stop = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train_stop - train_start) / 3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model,file_path):\n",
    "    f = h5py.File(file_path)\n",
    "\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for i, layer in enumerate(model.layers[:31]):\n",
    "        g = f[layer_names[i]]\n",
    "        weights = [g[name] for name in g.attrs['weight_names']]\n",
    "        layer.set_weigh\n",
    "        ts(weights)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    print('Pretrained Model weights loaded.')\n",
    "    \n",
    "def main():\n",
    "    style= style_image_path #args.style\n",
    "    #img_width = img_height =  args.image_size\n",
    "    output_file = 'test' #args.output\n",
    "    input_file = base_image_path #args.input\n",
    "    original_color = 0 #args.original_color\n",
    "    blend_alpha = 0 #args.blend\n",
    "    #media_filter = args.media_filter\n",
    "\n",
    "    aspect_ratio, x = img_util.preprocess_reflect_image(input_file, size_multiple=4)\n",
    "\n",
    "    img_width= img_height = x.shape[1]\n",
    "    net = image_transform_net(img_width, img_height)\n",
    "    model = vgg_loss_net(net.output,net.input,img_width,img_height,\"\",0,0)\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(optimizers.Adam(),  dummy_loss)  # Dummy loss since we are learning from regularizes\n",
    "\n",
    "    model.load_weights(\"weights/des_glaneuses_weights.h5\",by_name=False)\n",
    "\n",
    "    \n",
    "    t1 = time.time()\n",
    "    y = net.predict(x)[0]\n",
    "    \n",
    "    y = img_util.crop_image(y, aspect_ratio)\n",
    "    \n",
    "    print(\"process: %s\" % (time.time() -t1))\n",
    "\n",
    "    ox = img_util.crop_image(x[0], aspect_ratio)\n",
    "\n",
    "    # y =  median_filter_all_colours(y, media_filter)\n",
    "\n",
    "    if blend_alpha > 0:\n",
    "        y = blend(ox,y,blend_alpha)\n",
    "\n",
    "\n",
    "    if original_color > 0:\n",
    "        y = original_colors(ox,y,original_color )\n",
    "\n",
    "    showimg(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: 2.412416458129883\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2359296 into shape (400,400,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-629d385bc186>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mox\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moriginal_color\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mshowimg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-daab32c6417c>\u001b[0m in \u001b[0;36mshowimg\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# 將張量轉回圖片的後處理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# 取得原圖比例\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-e4d1d62f2451>\u001b[0m in \u001b[0;36mdeprocess_image\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMG_WIDTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_HEIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m103.939\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2359296 into shape (400,400,3)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
